{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3acfb9b",
   "metadata": {},
   "source": [
    "## Final Project E-Commerce: Product Range Analysis (Code)\n",
    "\n",
    "### Student: Yasmin Madjitey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447c637e",
   "metadata": {},
   "source": [
    "Project Goal:\n",
    "The main goal here is to analyze the store's product range with regards to profitability and customer demand.\n",
    "That is, a careful analysis to provide the basis for targeted adjustments to boost sales, hence profits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2ded8",
   "metadata": {},
   "source": [
    "`Link to presentation: https://docs.google.com/presentation/d/1AZO10liTidFQEU5HG9tAtl0utLLmsi3q7SZSwmW-c70/edit?usp=sharing`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705dfe4",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "### Carry out exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862d591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import networkx as nx\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying the zip file name where the dataset is stored\n",
    "file_name = \"ecommerce_dataset_us.zip\"\n",
    "  \n",
    "# opening the zip file in 'read' mode\n",
    "with ZipFile(file_name) as zip:\n",
    "    # printing all the contents of the zip file\n",
    "    zip.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d654fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the extracted csv file as a pandas dataframe\n",
    "df_original = pd.read_csv(\"ecommerce_dataset_us.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a glance at the data\n",
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1266d242",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a copy of the original dataset to df\n",
    "df = df_original.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aaeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740a40b",
   "metadata": {},
   "source": [
    "Note: \n",
    "- customer ID column is of type float instead of an object since aggregations cannot be done on customer ID. \n",
    "- Invoice date needs to be changed from object type to datetime.\n",
    "- customer id contains nan values\n",
    "- Quantity column has negative values\n",
    "- Price column has negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b84fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('UnitPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7795fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price should be > than 0\n",
    "df.loc[df['UnitPrice'] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9e7a8",
   "metadata": {},
   "source": [
    "Note:\n",
    "- These products seem to be damaged products which cannot be sold, hence they have no customer ids and cost nothing (0.0 unit price) as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32678c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['UnitPrice'] <= 0].sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90120d",
   "metadata": {},
   "source": [
    "Note:\n",
    "- realize that majority of the 0.00 price values have corresponding 'nan' customer id values, 'nan' descriptions and (-) quantity values\n",
    "- these could pass as damaged goods recorded by the store with no price values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a712e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the 0.00 priced values to nan values and visualize missingness in dataset\n",
    "df.loc[df['UnitPrice'] <= 0, 'UnitPrice'] = np.nan\n",
    "# test\n",
    "print(df.loc[df['UnitPrice'].isnull()])\n",
    "\n",
    "# to visualize data completeness (missing values)\n",
    "%matplotlib inline\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd26cc",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- they represent a very small fraction of the dataset, hence it is safe to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2550df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove these values since they would not be valuable to the analysis\n",
    "df = df.loc[~df['UnitPrice'].isnull()]\n",
    "# verify if the values have been correctly removed\n",
    "df.info()\n",
    "df.loc[df['UnitPrice'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding outliers by price\n",
    "outliers = df.groupby('Description').agg({'UnitPrice': 'mean'}).sort_values('UnitPrice', ascending = False)\n",
    "outliers.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705c994",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- Descriptions such as Adjust bad debt, AMAZON FEE, CRUK Commission, DOTCOM POSTAGE, Manual, SAMPLES, Bank Charges have average prices larger than the average unit price of the products. They do not seem to represent store products like the rest do. Furthermore, these items are not direct indicators of orders and might skew the sales distribution. They do not directly tie to individual product order, hence, they would be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_items = ['DOTCOM POSTAGE', 'Adjust bad debt', 'CRUK Commission', 'Bank Charges', 'AMAZON FEE', 'Manual',\n",
    "                'POSTAGE', 'SAMPLES']\n",
    "\n",
    "df = df.loc[~df['Description'].isin(remove_items)]\n",
    "\n",
    "#test\n",
    "df.loc[df['Description'].isin(remove_items)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Quantity'] <= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b2cc3",
   "metadata": {},
   "source": [
    "Note:\n",
    "- the negative quantity values with invoice numbers starting with 'C' look like returned or cancelled goods especially because they have corresponding customer ids.\n",
    "\n",
    "- Discount on the otherhand affect prices.\n",
    "\n",
    "- It is a great idea to get and remove both the original and returned items from the customers' orders.\n",
    "\n",
    "- Another way would be to keep them since running a .sum() on quantity would cancel out these items.\n",
    "\n",
    "- Let's see if there are some without customer ids first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a1644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Quantity'] <= 0) & (df['CustomerID'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade84e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Quantity'] <= 0) & (df['CustomerID'].isnull())].sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0c2ea",
   "metadata": {},
   "source": [
    "Note:\n",
    "- We do have cancelled or returned items without customer ids. One thing about these items is that they do represent store items. Some of the questions that come to mind include; were these items cancelled way before they were paid for by the customer, hence they do not have customer ids? were they probably cancelled by the store? or left in the customers' shopping cart until the alloted time lapsed?\n",
    "\n",
    "\n",
    "- Although these values constitute a small fraction of the dataset, they would not be removed since more information is needed to clear doubts. \n",
    "\n",
    "- Also because they represent losses to the store. removing them without justification would overstate the store's actual sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b84802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Quantity'] <= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.InvoiceNo.str.startswith('C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca26776",
   "metadata": {},
   "source": [
    "- We see that all the items with negative quantity values have the invoice number starting with 'C'. We can conclude that, all the negative quantities in our dataset are cancelled or returned items except for discount\n",
    "\n",
    "- Before moving on, let's visualize common returned items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_df = df.loc[df.InvoiceNo.str.startswith('C')]\n",
    "\n",
    "grouped = returned_df.groupby('Description')['InvoiceNo'].nunique().sort_values(ascending = False)\n",
    "print(grouped.head())\n",
    "\n",
    "fig = px.bar(data_frame = grouped.head(20))\n",
    "fig.update_layout(title = 'Frequently Returned or Cancelled Items')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239de2a0",
   "metadata": {},
   "source": [
    "Note:\n",
    "- 'REGENCY CAKESTAND 3 TIER' is the most returned store item.\n",
    "\n",
    "- 'Discount' in the dataset represents dicounts applied on products price (reduces product price), hence, the negative value. These would not be removed since it affects the price of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Description'] == 'Discount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pair of original and cancelled or returned orders\n",
    "#cancelled_orders = df.loc[df.groupby(['CustomerID', 'StockCode'])['Quantity'].transform('sum').eq(0)].sort_values(by=['CustomerID', 'StockCode'])\n",
    "#print(cancelled_orders)\n",
    "\n",
    "# drop these items:\n",
    "#get_index\n",
    "#get_index = cancelled_orders.index\n",
    "\n",
    "#df = df.drop(index = get_index)\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa40d3d",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- On a second thought, removing returned and the corresponding original orders is quiet tricky. What if discounts were applied on these items? removing them without their corresponding discounts would render the analysis biased.\n",
    "\n",
    "- It is also not clear how the store's discount policy works.\n",
    "\n",
    "- It is safer to leave them in the dataset since running a .sum() would cancel them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc616c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values in customer id column\n",
    "df['CustomerID'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d376cf3",
   "metadata": {},
   "source": [
    "Note:\n",
    "- I would not remove the nan values in the customer id column as it is a big amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting invoice date from object type to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "\n",
    "# converting customer ID column to object datatype\n",
    "df['CustomerID'] = df.CustomerID.astype('str')\n",
    "\n",
    "# test\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .0 at the end of the customer ID\n",
    "df['CustomerID'] = df.CustomerID.str[:-2]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CustomerID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c7ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['CustomerID'] == 'n', 'CustomerID'] = np.nan\n",
    "\n",
    "# test\n",
    "df[df['CustomerID'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c6afa",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeframe of data\n",
    "print(\"Time From:\")\n",
    "print(str(df.InvoiceDate.min()))\n",
    "print(\"To:\")\n",
    "print(str(df.InvoiceDate.max()))\n",
    "print('\\n')\n",
    "print(\"A year's data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b066bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sales column (quantity * unitprice)\n",
    "df['Sales'] = df['Quantity'] * df['UnitPrice']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba30ebbb",
   "metadata": {},
   "source": [
    "***Univariate analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20192893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Sales.describe())\n",
    "\n",
    "# plotting sales distribution before applying transformation\n",
    "df['Sales'].plot.hist(bins = 50, alpha = 0.5)\n",
    "plt.xlim(-10000,  10000)\n",
    "plt.title('Distribution of Sales; Untransformed Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4221a52",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- the minimum value in the sales data is '-168469.600000' and maximum is '168469.600000'. This must be a returned or cancelled item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting sales distribution after applying cuberoot transformation because of negative values in the dataset\n",
    "sales_cuberoot = np.cbrt(df['Sales'])\n",
    "sales_cuberoot.plot.hist(bins = 200, alpha = 0.5)\n",
    "plt.xlim(-5, 15)\n",
    "plt.title('Distribution of Sales: Cuberoot Transformed Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15908b13",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Cuberoot transformations reduce or remove skewness of a dataset especially where negative values are involved.\n",
    "\n",
    "- we see that the sales column has outliers. Since the store distributes on wholesale terms as well, these values are accounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e101e",
   "metadata": {},
   "source": [
    "***Bivariate Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b59925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily unique orders\n",
    "number_purchases_per_day = df.set_index('InvoiceDate').resample('D').nunique().reset_index()\n",
    "#print(number_purchases_per_day)\n",
    "\n",
    "fig = px.line(data_frame = number_purchases_per_day, x = 'InvoiceDate', y = 'InvoiceNo')\n",
    "fig.update_layout(title = \"Daily Unique or New Orders\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d44ee",
   "metadata": {},
   "source": [
    "Note:\n",
    "- From the graph, we can see that there are days where the store had 0 number of orders. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf8d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_purchases_per_day.loc[number_purchases_per_day['Quantity']== 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad8ac2",
   "metadata": {},
   "source": [
    "Note:\n",
    "- the dates here are additive by 7, could that replicate the last day of the week (Sunday) where most stores are closed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total purchases and sales per day\n",
    "total_purch_per_day = df.set_index('InvoiceDate').resample('D').sum().reset_index()\n",
    "print(total_purch_per_day)\n",
    "\n",
    "fig1 = px.line(data_frame = total_purch_per_day, x = 'InvoiceDate', y = 'Quantity')\n",
    "fig1.update_layout(title = \"Total Purchases per Day\")\n",
    "fig1.show()\n",
    "\n",
    "fig2 = px.line(data_frame = total_purch_per_day, x = 'InvoiceDate', y = 'Sales')\n",
    "fig2.update_layout(title = \"Total Sales per Day\")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53580593",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- We see that the store had big sales on Nov. 12 & Sep. 18, whereas the store had its biggest orders on these days including Oct. 3, which orders surpassed that on Sep. 18.\n",
    "\n",
    "- Larger quatities sold do not necessarily mean larger sales. there is a confounding variable here i.e. Unitprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c043be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly purchases (busiest hour)\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "df.Hour.unique()\n",
    "\n",
    "hour = [ 6, 7, 8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "# converting the hour data type to ordinal\n",
    "ordered_hour = pd.api.types.CategoricalDtype(ordered = True, categories = hour)\n",
    "df['Hour'] = df['Hour'].astype(ordered_hour)\n",
    "df.info()\n",
    "\n",
    "grouping_by_hour = df.groupby(['Hour'])['Quantity'].count().sort_values(ascending = False)\n",
    "print(grouping_by_hour)\n",
    "\n",
    "# visualize the busiest hours\n",
    "sns.lineplot(data = grouping_by_hour)\n",
    "plt.title(\"Orders by Hour\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6f810",
   "metadata": {},
   "source": [
    "Note:\n",
    "- From the diagram, our busiest hours are approximately 12pm and 3 pm\n",
    "- The store can increase advertisements around this time to pull in more customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062828ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily orders (busiest day)\n",
    "df['Weekday'] = df['InvoiceDate'].dt.day_name()\n",
    "print(df.Weekday.unique())\n",
    "\n",
    "day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "# converting the day data type to ordinal\n",
    "ordered_day = pd.api.types.CategoricalDtype(ordered = True, categories = day)\n",
    "df['Weekday'] = df['Weekday'].astype(ordered_day)\n",
    "df.info()\n",
    "\n",
    "grouping_by_day = df.groupby(['Weekday'])['Quantity'].count().sort_values(ascending = False).reset_index()\n",
    "print(grouping_by_day)\n",
    "\n",
    "# visualize the busiest days\n",
    "sns.barplot(data = grouping_by_day, x = 'Weekday', y= 'Quantity', color = sns.color_palette()[0])\n",
    "plt.title(\"Orders by Day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520d775",
   "metadata": {},
   "source": [
    "Note:\n",
    "- As imagined, our diagram did not contain data for Sunday. Friday represents our busiest day at the store.\n",
    "- the store can increase its advertisements on this day to increase orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc56a",
   "metadata": {},
   "source": [
    "***Multivariate Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cf353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationship between price, sales and quantity ordered\n",
    "print(total_purch_per_day)\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "sns.scatterplot(x = total_purch_per_day.UnitPrice, y = total_purch_per_day.Sales, hue= total_purch_per_day.Quantity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"correlation b/n price and sales: \", total_purch_per_day['UnitPrice'].corr(total_purch_per_day['Sales']))\n",
    "print(\"correlation b/n price and quantity: \", total_purch_per_day['UnitPrice'].corr(total_purch_per_day['Quantity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642c05d",
   "metadata": {},
   "source": [
    "Note:\n",
    "- the impact price has on sales is not so different from the impact price has on quantity bought. A bigger price could have orders with larger quantities. Price does not really make a difference here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c681c7c6",
   "metadata": {},
   "source": [
    "### Analyze the product range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a75b2",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Lets visualize some of the products we have in the store as well as top products from orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a900f73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_visualize = df.copy()\n",
    "df_visualize['all'] = 'all'\n",
    "fig = px.treemap(df_visualize.head(30), path=['all', \"Description\"], values='Quantity', \n",
    "                 color = df_visualize[\"Quantity\"].head(30), hover_data=['Description'], color_continuous_scale='Blues')\n",
    "fig.update_layout(title = 'Some Products at the Store')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97410c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize top 15 products from orders\n",
    "df_network = df.copy()\n",
    "df_network_first = df_network.groupby(\"Description\").sum().sort_values(\"Quantity\", ascending=False).reset_index()[:15]\n",
    "df_network_first[\"ItemType\"] = \"Store Products\"\n",
    "print(df_network_first)\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "first_choice = nx.from_pandas_edgelist(df_network_first, source='ItemType', target=\"Description\", edge_attr=True)\n",
    "pos = nx.spring_layout(first_choice)\n",
    "nx.draw_networkx_nodes(first_choice, pos, node_size=12500, node_color=\"lavender\")\n",
    "nx.draw_networkx_edges(first_choice, pos, width=2, alpha=0.5, edge_color='black')\n",
    "nx.draw_networkx_labels(first_choice, pos, font_size=12, font_family='sans-serif')\n",
    "plt.axis('off')\n",
    "plt.grid()\n",
    "plt.title('Top 15 Products', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10304494",
   "metadata": {},
   "source": [
    "### Formulate and test statistical hypotheses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c467375",
   "metadata": {},
   "source": [
    "***problem statement 1: How stable is sales? \n",
    "Hypotheses: can it be explained by demand variability or seasonality?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly sales\n",
    "monthly_sales = df.set_index('InvoiceDate').resample('M').sum().reset_index()\n",
    "print(monthly_sales)\n",
    "\n",
    "fig = px.line(data_frame = monthly_sales, x = 'InvoiceDate', y = 'Sales')\n",
    "fig.update_layout(title = \"Monthly Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ffb56c",
   "metadata": {},
   "source": [
    "Note 1:\n",
    "\n",
    "Peak sales were around November 2019. The same cannot be said for 2018. Why is that?\n",
    "\n",
    "- could it be that the store launched around November, 2018?\n",
    "\n",
    "- did the store introduce new products or strategies in November, 2019 which drove sales?\n",
    "\n",
    "***not enough data to dig deeper into these hypotheses.***\n",
    "\n",
    "Again, we see that sales is not stable over the months. Demand varies across the months and since the data covers a year's sales, unbiased conclusions can not be drawn with regards to seasonality. More data would be needed to conclude if sales is seasonal or not. \n",
    "\n",
    "\n",
    "`Conclusion: whatever strategy the store used during November 2019 (Christmas season) really drove sales.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1c060",
   "metadata": {},
   "source": [
    "Note 2:\n",
    "\n",
    "I would use the following formula to check for variability in sales:\n",
    "\n",
    "- Coefficient of Variation (CV) = Average Sales/Standard Deviation of Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales = df.Sales.mean()\n",
    "print('Average Sales: ', avg_sales)\n",
    "std_sales = df.Sales.std()\n",
    "print('Standard Deviation: ', std_sales)\n",
    "CV_sales = (std_sales/avg_sales)* 100\n",
    "print('Coefficient of Variation: ', CV_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c9e4c",
   "metadata": {},
   "source": [
    "Note:\n",
    "- A high coefficient of variation depicts unstable customer demand hence the store may be faced with forecasting complexities, workload peaks and planning and distribution challenges.\n",
    "\n",
    "`recall that the graphical distribution of sales did not show a normal distribution.`\n",
    "\n",
    "- For this kind of distribution, a cause analysis would provide better results than the statistical approach used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ea288",
   "metadata": {},
   "source": [
    "***problem statement 2: What items should the store invest in more? Hypotheses: Can this be explained by price (affordability)***\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "- Categorize products using RFM segmentation and label cohorts: very fast movers, fast movers, slow movers and very slow movers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9034df6",
   "metadata": {},
   "source": [
    "Using RFM segmentation on products made a lot of sense to me under this circumstance, in that, it encompasses all aspects (monetary, recency and frequency). It reduces bias here.\n",
    "\n",
    "for example: segmenting products based on sales only (monetary) will have items that are not frequently bought but have high prices or sales categorized as fast movers, which would be wrong. Should the store stock up on these products, warehouse spacing would be an issue here.\n",
    "\n",
    "RFM segmentation based on RFM score will output product categories that give the store a 'bit of everything' in terms of sales, recency and frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year, month and day\n",
    "df['InvoiceDay'] = df['InvoiceDate'].dt.year.astype(str) + \"-\" + df['InvoiceDate'].dt.month.astype(str) + \"-\" + df['InvoiceDate'].dt.day.astype(str)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Weekday', 'Hour'], axis = 1, inplace = True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceDay'] = pd.to_datetime(df['InvoiceDay'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e078d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # recall the time period\n",
    "print('Min : {}, Max : {}'.format(min(df.InvoiceDay), max(df.InvoiceDay)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ba552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pin the last date\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "snapshot_date = max(df.InvoiceDay) + datetime.timedelta(days=1)\n",
    "snapshot_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad5280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RFM values\n",
    "rfm = df.groupby('Description').agg({'UnitPrice': 'mean',\n",
    "    'InvoiceDate' : lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo' : 'count', \n",
    "    'Sales' : 'sum'})\n",
    "# rename the columns\n",
    "rfm.rename(columns = {'InvoiceDate' : 'Recency', \n",
    "                      'InvoiceNo' : 'Frequency', \n",
    "                      'Sales' : 'Monetary'}, inplace = True)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels and assign them to percentile groups \n",
    "r_labels = range(3, 0, -1)\n",
    "r_groups = pd.qcut(rfm['Recency'], q = 4, labels = r_labels, duplicates='drop')\n",
    "f_labels = range(1, 5)\n",
    "f_groups = pd.qcut(rfm.Frequency, q = 4, labels = f_labels)\n",
    "m_labels = range(1, 5)\n",
    "m_groups = pd.qcut(rfm.Monetary, q = 4, labels = m_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d3b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column for group labels\n",
    "rfm['R'] = r_groups.values\n",
    "rfm['F'] = f_groups.values\n",
    "rfm['M'] = m_groups.values\n",
    "# sum up the three columns\n",
    "rfm['RFM_Segment'] = rfm.apply(lambda x: str(x['R']) + str(x['F']) + str(x['M']), axis = 1)\n",
    "rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis = 1)\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average values for each RFM\n",
    "rfm_agg = rfm.groupby('RFM_Score').agg({\n",
    "    'Recency' : 'mean',\n",
    "    'Frequency' : 'mean',\n",
    "    'Monetary' : ['mean', 'count']\n",
    "})\n",
    "rfm_agg.round(1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels from total score\n",
    "score_labels = ['Very_slow_movers', 'Slow_movers', 'Fast_movers', 'Very_fast_movers']\n",
    "score_groups = pd.qcut(rfm.RFM_Score, q = 4, labels = score_labels)\n",
    "rfm['RFM_Level'] = score_groups.values\n",
    "rfm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaef31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.sort_values('RFM_Score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da04ef",
   "metadata": {},
   "source": [
    "***problem statement 3: Does price affect sellability?***\n",
    "\n",
    "This would be checked on the categorized products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_fast_movers = rfm.loc[rfm['RFM_Level'] == 'Very_fast_movers'].sort_values('Monetary', ascending = False).reset_index()\n",
    "print(very_fast_movers.head())\n",
    "\n",
    "#plotting top 20 very_fast_movers against average price\n",
    "ax = very_fast_movers[:20].plot('Description', 'Monetary', kind = 'bar', color = 'g', figsize = (10, 8))\n",
    "ax1 = ax.twinx()\n",
    "very_fast_movers[:20].plot('Description', 'UnitPrice', ax = ax1, color = 'r')\n",
    "\n",
    "plt.title('Very Fast Moving Items vs Avg Price')\n",
    "ax.set_xlabel('Item Description')\n",
    "ax.set_ylabel('Sales', color = 'g')\n",
    "ax1.set_ylabel('Avg Price', color = 'r')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a4d6a",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- 'REGENCY CAKESTAND 3 TIER': most purchased yet most returned store item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8528e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales = very_fast_movers.Monetary.mean()\n",
    "print('Average Sales for Very Fast movers: ', avg_sales)\n",
    "std_sales = very_fast_movers.Monetary.std()\n",
    "print('Standard Deviation for Very Fast movers: ', std_sales)\n",
    "CV_sales = (std_sales/avg_sales)* 100\n",
    "print('Coefficient of Variation: ', CV_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887182f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "very_fast_movers['Monetary'].plot.hist(bins = 50, alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562859e9",
   "metadata": {},
   "source": [
    "Note :\n",
    "\n",
    "- prices vary much across the items in this category. Prices fall below 10 except for 'REGENCY CAKESTAND 3 TIER' which has an average price above 10.\n",
    "\n",
    "- Also, the very fast-movers category has a varied demand with a coefficient of variation at 121%. There is a high variability in the data distribution. Although not a normal distribution, majority of the orders or demand are in the stable area. The store would have some planning and distribution challenges with this category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad6af4",
   "metadata": {},
   "source": [
    "***fast-moving items***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4dd15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_movers = rfm.loc[rfm['RFM_Level'] == 'Fast_movers'].sort_values('Monetary', ascending = False).reset_index()\n",
    "print(fast_movers.head())\n",
    "\n",
    "#plotting top 20 fast-movers against average price\n",
    "ax = fast_movers[:20].plot('Description', 'Monetary', kind = 'bar', color = 'g', figsize = (10, 8))\n",
    "ax1 = ax.twinx()\n",
    "fast_movers[:20].plot('Description', 'UnitPrice', ax = ax1, color = 'r')\n",
    "\n",
    "plt.title('Fast movers vs Avg Price')\n",
    "ax.set_xlabel('Item Description')\n",
    "ax.set_ylabel('Sales', color = 'g')\n",
    "ax1.set_ylabel('Avg Price', color = 'r')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales = fast_movers.Monetary.mean()\n",
    "print('Average fast movers Sales: ', avg_sales)\n",
    "std_sales = fast_movers.Monetary.std()\n",
    "print('Standard Deviation of fast movers: ', std_sales)\n",
    "CV_sales = (std_sales/avg_sales)*100\n",
    "print('Coefficient of Variation: ', CV_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_movers['Monetary'].plot.hist(bins = 100, alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091d707",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- We see that majority of the item prices fall below 10\n",
    "\n",
    "- this category has a low coefficient of variation (104%) compared to the very fast movers. this category has a varied distribution of data. planning and distribution challenges would still be faced here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cd43d",
   "metadata": {},
   "source": [
    "***Slow movers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf663d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_movers = rfm.loc[rfm['RFM_Level'] == 'Slow_movers'].sort_values('Monetary', ascending = False).reset_index()\n",
    "print(slow_movers.head())\n",
    "\n",
    "#plotting top 20 slow-movers against average price\n",
    "ax = slow_movers[:20].plot('Description', 'Monetary', kind = 'bar', color = 'g', figsize = (10, 8))\n",
    "ax1 = ax.twinx()\n",
    "slow_movers[:20].plot('Description', 'UnitPrice', ax = ax1, color = 'r')\n",
    "\n",
    "plt.title('Items with slow-sales vs Avg Price')\n",
    "ax.set_xlabel('Item Description')\n",
    "ax.set_ylabel('Sales', color = 'g')\n",
    "ax1.set_ylabel('Avg Price', color = 'r')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = slow_movers.Monetary.mean()\n",
    "print('Average sales for Slow movers: ', mean_sales)\n",
    "std_sales = slow_movers.Monetary.std()\n",
    "print('Standard Deviation for Slow movers: ', std_sales)\n",
    "CV_sales = (std_sales/mean_sales)*100\n",
    "print('Coefficient of Variation: ', CV_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55165a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_movers['Monetary'].plot.hist(bins = 20, alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981ae3f",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "-  This category has store items with prices above 100 and store items with prices slightly above 0.\n",
    "\n",
    "- it also has a higher coefficient of variation at 267%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc0317",
   "metadata": {},
   "source": [
    "***Very Slow movers***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_slow_movers = rfm.loc[rfm['RFM_Level'] == 'Very_slow_movers'].sort_values('Monetary', ascending = False).reset_index()\n",
    "print(very_slow_movers.head())\n",
    "\n",
    "#plotting top 20 very_slow-movers against average price\n",
    "ax = very_slow_movers[:20].plot('Description', 'Monetary', kind = 'bar', color = 'g', figsize = (10, 8))\n",
    "ax1 = ax.twinx()\n",
    "very_slow_movers[:20].plot('Description', 'UnitPrice', ax = ax1, color = 'r')\n",
    "\n",
    "plt.title('Items with very slow-sales vs Avg Price')\n",
    "ax.set_xlabel('Item Description')\n",
    "ax.set_ylabel('Sales', color = 'g')\n",
    "ax1.set_ylabel('Avg Price', color = 'r')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24698ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales = very_slow_movers.Monetary.mean()\n",
    "print('Average of Very Slow movers Sales: ', mean_sales)\n",
    "std_sales = very_slow_movers.Monetary.std()\n",
    "print('Standard Deviation of Very Slow movers: ', std_sales)\n",
    "CV_sales = (std_sales/mean_sales)*100\n",
    "print('Coefficient of Variation: ', CV_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae81f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_slow_movers['Monetary'].plot.hist(bins = 100, alpha = 0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a166e",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- Like the very fast movers, this category has prices below 10 except for 'Vintage Post Office Cabinet' which has a high price, above 60.\n",
    "\n",
    "\n",
    "`Conclusions on the categorization done:`\n",
    "\n",
    "- the store is a wholesale vendor because it has a very varied sales distribution. Or it probably sells durable items which have a varied demand, like gift items.\n",
    "\n",
    "- The store may have to deal with forecasting challenges as well as planning and distribution challenges for all categories.\n",
    "\n",
    "\n",
    "- Albeit, a cause analysis will provide much more informed findings than the statistical approaches used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184be61",
   "metadata": {},
   "source": [
    "***problem statement 4:  What products are most often sold together?\n",
    "Hypotheses: association rules***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using apriori\n",
    "# summing up multiple products within the same order\n",
    "df.head()\n",
    "df_grouped = df.groupby(['InvoiceNo','Description']).sum()['Quantity']\n",
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack the grouped dataframe\n",
    "df_basket = df_grouped.unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "\n",
    "# set values to 0s and 1s\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "    \n",
    "basket_sets = df_basket.applymap(encode_units)\n",
    "basket_sets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc99892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular products in the store\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.03, use_colnames=True).sort_values(by=['support'], ascending=False)\n",
    "frequent_itemsets.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6149dac",
   "metadata": {},
   "source": [
    "Note:\n",
    "- The support column indicates how frequently the item appears in the dataset i.e. how popular a product is in the shop\n",
    "\n",
    "This is telling us that 'WHITE HANGING HEART T-LIGHT HOLDER', 'JUMBO BAG RED RETROSPOT', 'REGENCY CAKESTAND 3 TIER' are the most popular items in this shop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f970c",
   "metadata": {},
   "source": [
    "#### Use the function, “association_rules” from, “mlxtend” to create the dataframe from, “frequent_itemsets” using the arguments metric, “lift”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\")\n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c835bc",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- Lift is a metric to measure the ratio of the confidence of products occurring together if they were statistically independent. E.g. how likely is another product purchased when purchasing a product, while controlling how popular the other product is. A lift score that is close to 1 indicates that the antecedent and the consequent are independent and occurrence of antecedent has no impact on occurrence of consequent. A Lift score that is greater than 1 indicates that the antecedent and consequent are dependent to each other, and the occurrence of antecedent has a positive impact on occurrence of consequent. A lift score that is smaller than 1 indicates that the antecedent and the consequent are substitute each other that means the existence of antecedent has a negative impact to consequent or visa versa.\n",
    "\n",
    "\n",
    "- A lift ratio larger than 1.0 implies that the relationship between the two products is more significant than would be expected if the two sets were independent. The larger the lift ratio, the more significant the association. “confidence” is how often the rule has been found to be true. In order words, how reliable this rule is as a percentage.\n",
    "\n",
    "\n",
    "- Therefore, what we are looking for are rules where the “lift” and “confidence” is highest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[(rules['lift'] > 1) & (rules['confidence'] >= 0.25) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c40d3",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- The “antecedent” is the product being purchased and the “consequent” is the product that is often purchased as well with their associated likelihood.\n",
    "\n",
    "- This is telling us that in this shop if someone buys a “ROSES REGENCY TEACUP AND SAUCER\", they are likely to buy “GREEN REGENCY TEACUP AND SAUCER”  as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa574d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "During my research on the topic, the following websites enlightened me on some of the questions a data analyst should aim to answer when conducting product range analysis and how to go about some of the tasks.\n",
    "\n",
    "https://towardsdatascience.com/product-segmentation-for-retail-with-python-c85cc0930f9a (for general research purposes)\n",
    "\n",
    "https://towardsdatascience.com/shop-order-analysis-in-python-ff13615404e0 (for general research purposes)\n",
    "\n",
    "https://medium.com/swlh/product-sales-analysis-using-python-863b29026957 (for general research purposes)\n",
    "\n",
    "https://sunscrapers.com/blog/sales-data-science-a-step-by-step-guide-to-competitor-analysis-using-python/ (for general research purposes)\n",
    "\n",
    "https://stackoverflow.com/questions/60558920/looking-for-positive-values-that-match-negative-values-within-a-column-pandas (assisted me on how to get both the original and returned goods in a dataframe)\n",
    "\n",
    "https://stackoverflow.com/questions/46063379/pandas-secondary-axis (assisted me on how to plot on a secondary axis)\n",
    "\n",
    "https://github.com/finnqiao/cohort_online_retail/blob/master/ukretail_cohort.ipynb\n",
    "\n",
    "https://www.kdnuggets.com/2019/05/golden-goose-cohort-analysis.html (assisted me on how to apply RFM segmentation which I used on the products instead of customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa308773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
